{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question\n",
    "\n",
    "**Can Pointnet neural networks be used to identify timeslices with neutrino events?**\n",
    "\n",
    "Yes, it can be used, after several modifications (see PointNet Architecture below)\n",
    "\n",
    "**Can it perform with sufficient accuracy, and recall?**\n",
    "\n",
    "As of now, it does not meet the industry requirements. However, since there is no such previous work, \n",
    "the results of this thesis will set a benchmark for how well pointnet can perform\n",
    "\n",
    "**Can energy properties be inferred from the network?**\n",
    "\n",
    "No. Pointnet cannot be used for regression, as it is a classification network\n",
    "\n",
    "# The Pipeline\n",
    "\n",
    "**Data pre-processing** ---> **Data Sampling** ----> **Pointcloud Generation** ----> **3D Mesh Generation** ----> **Training** ----> **Evaluation**\n",
    "        \n",
    "# Data Pre-processing \n",
    "\n",
    "1. Addition of timeslice groups [Relevant Notebooks](./data_generation/)\n",
    "\n",
    "The dataset is divided into timeslice groups based on time chunks of 150000 nanoseconds. Each timeslice group contains xyz and time values, aside from other metadata. This will allow the network will identify if the timeslice contains neutrino hits or not. \n",
    "\n",
    "2. Removal of irrelevant columns [Relevant Notebook](./data_preparation/0.0_dataset_reduction.ipynb)\n",
    "\n",
    "# Data Sampling \n",
    "#### [Relevant Notebook](./ensemble/sampled/1.0_xyt_data_preparation_separate_sampling.ipynb)\n",
    "\n",
    "1. Generate classes:\n",
    "Separate timeslice groups into two classes:\n",
    "    1. `Class 0` groups that have only noise points\n",
    "    2. `Class 1` groups that have both noise and hits (referenced as mixed groups)\n",
    "\n",
    "2. For each class, separate into train and test groups\n",
    "Groups are further split into train and test groups\n",
    "\n",
    "3. Groups identified as test groups are directly saved\n",
    "This is because test data must not be tocuhed and manipulated in any way\n",
    "\n",
    "4. Groups identified as training groups are further sampled\n",
    "There is severe class imbalance within groups in `Class 1` that contains both hits and noise points. This can severely impact training performance. For this, the data is further sampled.\n",
    "\n",
    "\n",
    "# PointCloud Generation\n",
    "#### [Relevant Notebook](./ensemble/sampled/1.0_xyt_data_preparation_separate_sampling.ipynb)\n",
    "\n",
    "1. Save the relevant pointclouds\n",
    "Each timeslice group is saved as individual `group_#.xyz` file\n",
    "\n",
    "# 3D Mesh Generation\n",
    "#### [Relevant Notebook](./ensemble/sampled/mesh_generation.ipynb)\n",
    "\n",
    "1. Compute normals for each timeslice group: \n",
    "Normals are oriented  with respect to the input point cloud if normals exist. Next, converts float64 numpy array of shape (n, 3) to Open3D format. Normals are required to generate meshes.\n",
    "\n",
    "2. Generate Poission Mesh: (See Screened Poission Reconstruction for more information below)\n",
    "Implements the Screened Poisson Reconstruction proposed in Kazhdan and Hoppe, \"Screened Poisson Surface Reconstruction\", 2013. See https://github.com/mkazhdan/PoissonRecon\n",
    "\n",
    "3. Save as `.off` files\n",
    "The network specifically handles `.off` files\n",
    "    \n",
    "#### Screened Poisson Reconstruction\n",
    "It uses an approach known as an implicit meshing method, which is trying to “envelop” the data in a smooth cloth. We try to fit a watertight surface from the original point set by creating an entirely new point set representing an isosurface linked to the normals. There are several parameters available that affect the result of the meshing:\n",
    "\n",
    "1. Depth:\n",
    "Tree-depth is used for the reconstruction. The higher, the more detailed the mesh. With noisy data you keep vertices in the generated mesh that are outliers but the algorithm doesn’t detect them as such. So a low value (maybe between 5 and 7) provides a smoothing effect, but you will lose detail. The higher the depth-value the higher is the resulting amount of vertices of the generated mesh.\n",
    "\n",
    "2. Width: \n",
    "This specifies the target width of the finest level of the tree structure, which is called an octree. D\n",
    "\n",
    "3. Scale:\n",
    "It describes the ratio between the diameter of the cube used for reconstruction and the diameter of the samples’ bounding cube. Very abstract, the default parameter usually works well.\n",
    "\n",
    "4. Fit:\n",
    "The linear_fit parameter if set to true, let the reconstructor use linear interpolation to estimate the positions of iso-vertices.\n",
    "\n",
    "\n",
    "\n",
    "# Organisation of files \n",
    "PointNet NN requires data to be made available in the following scheme:\n",
    "- `class1`  \n",
    "    - `train`\n",
    "        * file.off\n",
    "        * file.off \n",
    "    - `test` \n",
    "        * file.off\n",
    "        * file.off\n",
    "- `class2`\n",
    "    - train \n",
    "        * file.off\n",
    "        * file.off    \n",
    "    - `test`  \n",
    "        * file.off\n",
    "        * file.off\n",
    "        \n",
    "# PointNet Architecture \n",
    "\n",
    "Requirements for point cloud data:\n",
    "\n",
    "1. Point clouds should be unordered. Algorithm has to be invariant to permutations of the input set.\n",
    "2. Network must be invariant to rigid transformations.\n",
    "3. Network should capture interactions among points.\n",
    "\n",
    "Following are the modifications made to PointNet to work with KM3Net Data:\n",
    "\n",
    "1. Mapping `.off files`\n",
    "\n",
    "The dataset consists of .off files that contain meshes represented by vertices and triangular faces. Vertices are points in a 3D space and each triangle is formed by 3 vertex indices.\n",
    "\n",
    "\n",
    "2. Point sampling (As per https://github.com/fxia22/pointnet.pytorch)\n",
    "\n",
    "As points are not uniformly distributed across object’s surface, it will be difficult for  PointNet to classify them. \n",
    "Points are therefore uniformly sampled on the object’s surface. Faces can have different areas and hence we may assign probability of choosing a particular face proportionally to its area. \n",
    "As the network will have dense layers in the architecture, a fixed number of points are required per point cloud. For this, faces are sampled from the constructed distribution. After that, one point per chosen face gets sampled.\n",
    "\n",
    "3. Augmentations\n",
    "That pointclouds can have different sizes and can be placed in different parts of the coordinate system.\n",
    "So, they are translated to the origin by subtracting mean from all its points and normalizing its points into a unit sphere. To augment the data during training, we randomly rotate objects around Z-axis and add Gaussian noise as described in the original paper.\n",
    "\n",
    "4. Model\n",
    "The key point is that the result should be invariant to input points permutations and geometric transformations, such as rigid transformations.\n",
    "    1. First tensors will have size (batch_size, num_of_points, 3). In this case MLP with shared weights is just 1-dim convolution with a kernel of size 1.\n",
    "    2. To ensure invariance to transformations, apply the 3x3 transformation matrix predicted by T-Net to coordinates of input points. It is not possible to encode translations in 3D space by a 3-dimensional matrix. This is therefore taken care of by translating point clouds to the origin during pre-processing.\n",
    "    3. For initialisation of the output matrix, it should be an identity matrix by default to start training with no transformations at all. So, an identity matrix is added to the output. Additionally, the same but 64-dim T-Net is used to align extracted point features after applying MLP.\n",
    "    4. To provide permutation invariance, a symmetric function (max pooling) is applyed to the extracted and transformed features so the result does not depend on the order of input points anymore.\n",
    "    5. Loss is chosen to be NLLoss() with Log Sigmoid activation function (based on experiments mentioned below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments:\n",
    "\n",
    "Due to the exploratory nature of the research question, i.e. can PointNets work with such a problem, several experiments were carries out in phases.\n",
    "\n",
    "1. Meshes`xyz` -> Experiment stopped based on feedback from progress meeting\n",
    "\n",
    "2. Points`xyz` -> Experiment stopped for the same reason as above\n",
    "The Network can also work with just points and not 3D meshes. But it is unable to learn much information\n",
    "\n",
    "3. Ensemble of Meshes `xyt`, `xzt` `yzt` --> Finalised Setup\n",
    "Here `time` is made part of the dataset as per Physics requirements\n",
    "\n",
    "**Best Accuraccy Overall: 66%**\n",
    "\n",
    "**Recall (Class1/Class0): 40%/90%**\n",
    "\n",
    "**Loss: 0.001**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Ideas\n",
    "\n",
    "1. LSTM Units\n",
    "2. Adding 4D points to Pointnet (time as a feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.0:\n",
    "\n",
    "**Date:** 17-July-2020\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "**Train Results:**\n",
    "\n",
    "**Test Results:**\n",
    "\n",
    "**Comments:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem At Hand\n",
    "(Identified on 22-August-2020)\n",
    "\n",
    "When sampling 6550 points only per timeslice for both train and test ie. using a small subset of data per timeslice, there is distinct difference between the two types of classes, allowing for the classifier to classify with great accuraccy.\n",
    "\n",
    "<img src=\"../assets/experiments/problem/noise_6550.png\">\n",
    "<img src=\"../assets/experiments/problem/mixed_6550.png\">\n",
    "\n",
    "However, without this sampling, the meshes look very similar to each other. \n",
    "<img src=\"../assets/experiments/problem/noise_unsampled.png\">\n",
    "<img src=\"../assets/experiments/problem/mixed_unsampled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Mesh Algorithms\n",
    "\n",
    "### Problem:\n",
    "Based on images above, it is clear that timeslices as a whole do not show enough distinction between mixed and noise. This is solely due to the Poission Mesh Algorithm. \n",
    "\n",
    "\n",
    "### Requirements:\n",
    "Find a mesh a;gorithm that can suitably identify these distinct features\n",
    "\n",
    "\n",
    "### Options:\n",
    "1 BPA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "\n",
    "GPUs = GPU.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm() ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Meshes `xyz`\n",
    "\n",
    "#### Exp 1.0:\n",
    "**Date:** 09-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. logsoftmax\n",
    "\n",
    "**Train Results (Best Score):**\n",
    "accuraccy: 50 % \n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_1.0.png\">\n",
    "\n",
    "#### Exp 2.0:\n",
    "**Date:** 10-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: LogSigmoid** \n",
    "\n",
    "**Train Results:**\n",
    "accuraccy: 59 %\n",
    "\n",
    "**Test Results:**\n",
    " <img src=\"../assets/experiments/exp_2.0.png\">\n",
    "\n",
    "##### Exp 2.1:\n",
    "**Date:** 10-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: Sigmoid** \n",
    "\n",
    "**Rationale**\n",
    "Does changing between log sigmoid vs sigmoid have any effect on results?\n",
    "It actually decreased training performance\n",
    "\n",
    "**Train Results:**\n",
    "accuraccy: 50 %\n",
    "\n",
    "**Test Results:**\n",
    " <img src=\"../assets/experiments/exp_2.1.png\">\n",
    "\n",
    "#### Exp 3.0: \n",
    "**Date:** 11-Jul-2020\n",
    "**Results:**\n",
    "1024 Max points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. **Changed:LossFunction - BinaryEntropyLoss**\n",
    "\n",
    "**Rationale**\n",
    "The loss function for predicting binary outcomes should be Binary Loss Entropy\n",
    "\n",
    "**Result**\n",
    "No real difference between negative log likelihood loss mathematically\n",
    "\n",
    "##### Exp 3.1:\n",
    "**Date:** 11-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 Max points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. **Changed:LossFunction - CrossEntropyLoss**\n",
    "\n",
    "**Rationale**\n",
    "Binary Loss Entropy did not work. Tried Crossentropy (multiclass as binary problem)\n",
    "\n",
    "**Train Results:**\n",
    "accuraccy: 54 %\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_3.1.png\">\n",
    "\n",
    "#### Exp 4.0:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters: Evaluating a Larger Model**\n",
    "\n",
    "**Changed: Max points to 2048**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "accuraccy: 50 %\n",
    "\n",
    "**Remarks**\n",
    "No real difference\n",
    "\n",
    "**Test Results:**\n",
    "<!-- <img src=\"../assets/experiments/\"> -->\n",
    "\n",
    "#### Exp 5.0:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "**Changed: Increased files to 200 per type**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "accuraccy: 60 %\n",
    "loss: 0.009\n",
    "\n",
    "\n",
    "**Remarks**\n",
    "Produced 60% validation scores. Loss decrease rate stabalises around 10-11 epochs\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_5.0.png\">\n",
    "\n",
    "\n",
    "##### Exp 5.1:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "200 Files/Class\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: Sigmoid **\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "accuraccy: 60 %\n",
    "\n",
    "\n",
    "**Rationale**\n",
    "Sigmoid produced a more precise confusion matrix despite lower scores\n",
    "\n",
    "\n",
    "**Remarks**\n",
    "Produced 60% validation scores. Loss decrease rate stabalises around 10-11 epochs\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_5.0.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.0:\n",
    "\n",
    "**Date:** 09-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. logsoftmax\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 40 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_1.0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Exp 2.0:\n",
    "\n",
    "**Date:** 10-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: LogSigmoid** \n",
    "\n",
    "**Train Results:**\n",
    "50 %, 40 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 54 %, 50 %, 59 %\n",
    "\n",
    "**Test Results:**\n",
    " <img src=\"../assets/experiments/exp_2.0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exp 2.1:\n",
    "\n",
    "**Date:** 10-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: Sigmoid** \n",
    "\n",
    "**Rationale**\n",
    "\n",
    "Does changing between log sigmoid vs sigmoid have any effect on results?\n",
    "It actually decreased training performance\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 40 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 45 %, 45 %\n",
    "\n",
    "**Test Results:**\n",
    " <img src=\"../assets/experiments/exp_2.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Exp 3.0: \n",
    "**Date:** -Jul-2020\n",
    "\n",
    "**Results:**\n",
    "1024 Max points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. **Changed:LossFunction - BinaryEntropyLoss**\n",
    "\n",
    "**Rationale**\n",
    "The loss function for predicting binary outcomes should be Binary Loss Entropy\n",
    "\n",
    "**Result**\n",
    "\n",
    "No real difference between negative log likelihood loss mathematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exp 3.1:\n",
    "**Date:** -Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 Max points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. **Changed:LossFunction - CrossEntropyLoss**\n",
    "\n",
    "**Rationale**\n",
    "Binary Loss Entropy did not work. Tried Crossentropy (multiclass as binary problem)\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 54 %, 54 %\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_3.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 4.0:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters: Evaluating a Larger Model**\n",
    "**Changed: Max points to 2048**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 40 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %\n",
    "\n",
    "**Remarks**\n",
    "No real difference\n",
    "\n",
    "**Test Results:**\n",
    "<!-- <img src=\"../assets/experiments/\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Exp 5.0:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "**Changed: Increased files to 200 per type**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 50 %, 50 %, 55 %, 43 %, 45 %, 53 %, 60 %, 50 %, 50 %, 47 %, 47 %, 50 %, 47 %, 46 %\n",
    "\n",
    "\n",
    "[Epoch: 1, Batch:   10 /   10], loss: 0.569\n",
    "\n",
    "[Epoch: 2, Batch:   10 /   10], loss: 0.243\n",
    "\n",
    "[Epoch: 3, Batch:   10 /   10], loss: 0.126\n",
    "\n",
    "[Epoch: 4, Batch:   10 /   10], loss: 0.074\n",
    "\n",
    "[Epoch: 5, Batch:   10 /   10], loss: 0.050\n",
    "\n",
    "[Epoch: 6, Batch:   10 /   10], loss: 0.038\n",
    "\n",
    "[Epoch: 7, Batch:   10 /   10], loss: 0.028\n",
    "\n",
    "[Epoch: 8, Batch:   10 /   10], loss: 0.022\n",
    "\n",
    "[Epoch: 9, Batch:   10 /   10], loss: 0.019\n",
    "\n",
    "[Epoch: 10, Batch:   10 /   10], loss: 0.016\n",
    "\n",
    "[Epoch: 11, Batch:   10 /   10], loss: 0.014\n",
    "\n",
    "[Epoch: 12, Batch:   10 /   10], loss: 0.013\n",
    "\n",
    "[Epoch: 13, Batch:   10 /   10], loss: 0.011\n",
    "\n",
    "[Epoch: 14, Batch:   10 /   10], loss: 0.010\n",
    "\n",
    "[Epoch: 15, Batch:   10 /   10], loss: 0.009\n",
    "\n",
    "\n",
    "**Remarks**\n",
    "Produced 60% validation scores. Loss decrease rate stabalises around 10-11 epochs\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_5.0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exp 5.1:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "200 Files/Class\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: Sigmoid **\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 50 %, 50 %, 55 %, 43 %, 45 %, 53 %, 60 %, 50 %, 50 %, 47 %, 47 %, 50 %, 47 %, 46 %\n",
    "\n",
    "\n",
    "**Rationale**\n",
    "Sigmoid produced a more precise confusion matrix despite lower scores\n",
    "\n",
    "\n",
    "**Remarks**\n",
    "Produced 60% validation scores. Loss decrease rate stabalises around 10-11 epochs\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_5.0.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.0:\n",
    "\n",
    "**Date:** 09-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. logsoftmax\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 40 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_1.0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 2.0:\n",
    "\n",
    "**Date:** 10-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: LogSigmoid** \n",
    "\n",
    "**Train Results:**\n",
    "50 %, 40 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 54 %, 50 %, 59 %\n",
    "\n",
    "**Test Results:**\n",
    " <img src=\"../assets/experiments/exp_2.0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exp 2.1:\n",
    "\n",
    "**Date:** 10-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: Sigmoid** \n",
    "\n",
    "**Rationale**\n",
    "\n",
    "Does changing between log sigmoid vs sigmoid have any effect on results?\n",
    "It actually decreased training performance\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 40 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 45 %, 45 %\n",
    "\n",
    "**Test Results:**\n",
    " <img src=\"../assets/experiments/exp_2.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 3.0: \n",
    "**Date:** -Jul-2020\n",
    "\n",
    "**Results:**\n",
    "1024 Max points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. **Changed:LossFunction - BinaryEntropyLoss**\n",
    "\n",
    "**Rationale**\n",
    "The loss function for predicting binary outcomes should be Binary Loss Entropy\n",
    "\n",
    "**Result**\n",
    "\n",
    "No real difference between negative log likelihood loss mathematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exp 3.1:\n",
    "**Date:** -Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "1024 Max points\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. **Changed:LossFunction - CrossEntropyLoss**\n",
    "\n",
    "**Rationale**\n",
    "Binary Loss Entropy did not work. Tried Crossentropy (multiclass as binary problem)\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 54 %, 54 %\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_3.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 4.0:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters: Evaluating a Larger Model**\n",
    "**Changed: Max points to 2048**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 40 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %, 50 %\n",
    "\n",
    "**Remarks**\n",
    "No real difference\n",
    "\n",
    "**Test Results:**\n",
    "<!-- <img src=\"../assets/experiments/\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 5.0:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "**Changed: Increased files to 200 per type**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 50 %, 50 %, 55 %, 43 %, 45 %, 53 %, 60 %, 50 %, 50 %, 47 %, 47 %, 50 %, 47 %, 46 %\n",
    "\n",
    "\n",
    "[Epoch: 1, Batch:   10 /   10], loss: 0.569\n",
    "\n",
    "[Epoch: 2, Batch:   10 /   10], loss: 0.243\n",
    "\n",
    "[Epoch: 3, Batch:   10 /   10], loss: 0.126\n",
    "\n",
    "[Epoch: 4, Batch:   10 /   10], loss: 0.074\n",
    "\n",
    "[Epoch: 5, Batch:   10 /   10], loss: 0.050\n",
    "\n",
    "[Epoch: 6, Batch:   10 /   10], loss: 0.038\n",
    "\n",
    "[Epoch: 7, Batch:   10 /   10], loss: 0.028\n",
    "\n",
    "[Epoch: 8, Batch:   10 /   10], loss: 0.022\n",
    "\n",
    "[Epoch: 9, Batch:   10 /   10], loss: 0.019\n",
    "\n",
    "[Epoch: 10, Batch:   10 /   10], loss: 0.016\n",
    "\n",
    "[Epoch: 11, Batch:   10 /   10], loss: 0.014\n",
    "\n",
    "[Epoch: 12, Batch:   10 /   10], loss: 0.013\n",
    "\n",
    "[Epoch: 13, Batch:   10 /   10], loss: 0.011\n",
    "\n",
    "[Epoch: 14, Batch:   10 /   10], loss: 0.010\n",
    "\n",
    "[Epoch: 15, Batch:   10 /   10], loss: 0.009\n",
    "\n",
    "\n",
    "**Remarks**\n",
    "Produced 60% validation scores. Loss decrease rate stabalises around 10-11 epochs\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_5.0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exp 5.1:\n",
    "**Date:** 13-Jul-2020\n",
    "\n",
    "**Parameters:**\n",
    "200 Files/Class\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Changed: Sigmoid **\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "50 %, 50 %, 50 %, 55 %, 43 %, 45 %, 53 %, 60 %, 50 %, 50 %, 47 %, 47 %, 50 %, 47 %, 46 %\n",
    "\n",
    "\n",
    "**Rationale**\n",
    "Sigmoid produced a more precise confusion matrix despite lower scores\n",
    "\n",
    "\n",
    "**Remarks**\n",
    "Produced 60% validation scores. Loss decrease rate stabalises around 10-11 epochs\n",
    "\n",
    "**Test Results:**\n",
    "<img src=\"../assets/experiments/exp_5.0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Meshes `XYT XZT YZT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.0: \n",
    "**Date:** 05-August-2020\n",
    "\n",
    "**Condition:** 6550 points per timeslice were taken. Hits ordered first so\n",
    "that they would be selected first and the balance would be noise points\n",
    "\n",
    "**Parameters:**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "[Epoch: 14, Batch:   10 /   10], loss: 0.006\n",
    "Valid accuracy: 97 %\n",
    "\n",
    "**Classification report**\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "       mixed       1.00      0.93      0.96        40\n",
    "       noise       0.93      1.00      0.96        40\n",
    "\n",
    "    accuracy                           0.96        80\n",
    "    macro avg       0.97      0.96     0.96        80\n",
    "    weighted avg    0.97      0.96     0.96        80\n",
    "\n",
    "\n",
    "**Remarks**\n",
    "Very high accuraccy, precision and recall. However, this is on a very small subset of data. \n",
    "\n",
    "<!-- <img src=\"../assets/experiments/ensemble/exp_1.0.png\"> -->\n",
    "![alt text](../assets/experiments/ensemble/exp_1.0.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.1: \n",
    "**Date:** 05-August-2020\n",
    "\n",
    "**Condition**\n",
    "\n",
    "Test data must be a large set, to mimic real world scenario\n",
    "\n",
    "1. Split to train/test\n",
    "2. Train is resampled down to 6550 pints only\n",
    "3. Test is left untouched\n",
    "\n",
    "\n",
    "**Parameters:**\n",
    "1. Sampled, normalised, 6550 points\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "[Epoch: 14, Batch:   10 /   10], loss: 0.009\n",
    "Valid accuracy: 50 %\n",
    "\n",
    "**Classification report**\n",
    "\n",
    "Classification report for Pointnet:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "       mixed       0.29      0.10      0.15        40\n",
    "       noise       0.45      0.75      0.57        40\n",
    "\n",
    "    accuracy                           0.42        80\n",
    "       macro avg       0.37      0.42      0.36        80\n",
    "    weighted avg       0.37      0.42      0.36        80\n",
    "\n",
    "<img src=\"../assets/experiments/ensemble/exp_1.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.2: \n",
    "**Date:** 08-August-2020\n",
    "\n",
    "**Condition**\n",
    "6550 train, all test\n",
    "\n",
    "\n",
    "1. Split to train/test\n",
    "2. Train is resampled down to 6550 pints only\n",
    "3. Test is left untouched\n",
    "\n",
    "**Parameters:**\n",
    "1. Sampled, normalised, 1024 points \n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "[Epoch: 10, Batch:   10 /   10], loss: 0.010\n",
    "Valid accuracy: 61 %\n",
    "\n",
    "\n",
    "**Classification report**\n",
    "\n",
    "\n",
    "Classification report for Pointnet:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "       mixed       0.60      0.23      0.33        40\n",
    "       noise       0.52      0.85      0.65        40\n",
    "\n",
    "    accuracy                           0.54        80\n",
    "   macro avg       0.56      0.54      0.49        80\n",
    "weighted avg       0.56      0.54      0.49        80\n",
    "\n",
    "<img src=\"../assets/experiments/ensemble/exp_1.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.3: \n",
    "**Date:** 08-August-2020\n",
    "\n",
    "**Condition**\n",
    "6550 train, all test\n",
    "\n",
    "\n",
    "1. Split to train/test\n",
    "2. Train is resampled down to 6550 pints only\n",
    "3. Test is left untouched\n",
    "\n",
    "**Parameters:**\n",
    "1. Sampled, normalised, 1024 points \n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Sigmoid**\n",
    "5. **BCELoss**\n",
    "5. **30 epochs**\n",
    "\n",
    "**Train Results:**\n",
    "Valid accuracy: 55 %\n",
    "[Epoch: 23, Batch:   10 /   10], loss: 0.114\n",
    "\n",
    "Also, loss was fluctuating and never settled into minima\n",
    "\n",
    "**Classification report**\n",
    "\n",
    "\n",
    "Classification report for Pointnet:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "       mixed       0.52      0.60      0.56        40\n",
    "       noise       0.53      0.45      0.49        40\n",
    "\n",
    "    accuracy                           0.53        80\n",
    "    macro avg       0.53      0.53     0.52        80\n",
    "    weighted avg    0.53      0.53     0.52        80\n",
    "\n",
    "<img src=\"../assets/experiments/ensemble/exp_1.3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.4: \n",
    "**Date:** 08-August-2020\n",
    "\n",
    "**Condition**\n",
    "6550 train, all test\n",
    "\n",
    "\n",
    "1. Split to train/test\n",
    "2. Train is resampled down to 6550 pints only\n",
    "3. Test is left untouched\n",
    "\n",
    "**Parameters:**\n",
    "1. Sampled, normalised, 1024 points \n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **BCEWithLogitsLoss**\n",
    "5. **60 epochs**\n",
    "\n",
    "**Train Results:**\n",
    "[Epoch: 12, Batch:   10 /   10], loss: 0.130\n",
    "Valid accuracy: 65 %\n",
    "\n",
    "Also, loss was fluctuating and never settled into minima\n",
    "\n",
    "**Classification report**\n",
    "\n",
    "Classification report for Pointnet:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "       mixed       0.52      0.35      0.42        40\n",
    "       noise       0.51      0.68      0.58        40\n",
    "\n",
    "    accuracy                           0.51        80\n",
    "    macro avg       0.51      0.51     0.50        80\n",
    "    weighted avg    0.51      0.51     0.50        80\n",
    "\n",
    "<img src=\"../assets/experiments/ensemble/exp_1.4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 1.5: \n",
    "**Date:** 08-August-2020\n",
    "\n",
    "**Condition**\n",
    "6550 train, all test\n",
    "\n",
    "\n",
    "1. Split to train/test\n",
    "2. Train is resampled down to 6550 pints only\n",
    "3. Test is left untouched\n",
    "\n",
    "**Parameters:**\n",
    "1. Sampled, normalised, 1024 points \n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. **Cross Entropy Loss (includes sigmoid)**\n",
    "5. **80 epochs**\n",
    "\n",
    "**Train Results:**\n",
    "[Epoch: 59, Batch:   10 /   10], loss: 0.708\n",
    "Valid accuracy: 65 %\n",
    "\n",
    "Also, loss was fluctuating and never settled into minima (0.710-0.687)\n",
    "\n",
    "**Classification report**\n",
    "\n",
    "Classification report for Pointnet:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "        mixed       0.80      0.40      0.53        40\n",
    "        noise       0.60      0.90      0.72        40\n",
    "\n",
    "     accuracy                           0.65        80\n",
    "    macro avg       0.70      0.65      0.63        80\n",
    "    weighted avg    0.70      0.65      0.63        80\n",
    "\n",
    "<img src=\"../assets/experiments/ensemble/exp_1.5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 2.0: \n",
    "**Date:** 05-August-2020\n",
    "\n",
    "**Condition:** No points sampled\n",
    "\n",
    "**Parameters:**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. LogSigmoid\n",
    "5. NNLoss\n",
    "\n",
    "**Train Results:**\n",
    "[Epoch: 14, Batch:   10 /   10], loss: 0.009\n",
    "Valid accuracy: 57 %\n",
    "\n",
    "**Classification report**\n",
    "\n",
    "               Classification report for Pointnet:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "       mixed       0.38      0.15      0.21        40\n",
    "       noise       0.47      0.75      0.58        40\n",
    "\n",
    "    accuracy                           0.45        80\n",
    "    macro avg       0.42      0.45     0.40        80\n",
    "    weighted avg    0.42      0.45     0.40        80\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../assets/experiments/ensemble/exp_2.0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 3.0: \n",
    "**Date:** 08-August-2020\n",
    "\n",
    "**Condition:** Classes set to **k = 1** (instead of k = 2)\n",
    "\n",
    "**Parameters:**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "4. Sigmoid\n",
    "5. BCE\n",
    "6. 80 epochs\n",
    "\n",
    "**Train Results:**\n",
    "No improvements\n",
    "\n",
    "**Classification report**\n",
    "No improvements      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 4.0: Ensemble Best Loss Functiom\n",
    "\n",
    "Choosing the best loss function: BCEWith Logits, NLLLoss, CrossEntropyLoss\n",
    "\n",
    "**Date:** 15-August-2020\n",
    "\n",
    "No 6550 points, full data\n",
    "\n",
    "**Condition:** \n",
    "Batch:[32, 64]\n",
    "Epoch: 250    \n",
    "    \n",
    "**Parameters:**\n",
    "1. Sampled, normalised,\n",
    "2. Rotated\n",
    "3. Added Noise\n",
    "\n",
    "\n",
    "                   BCEWLogitsLoss     NLLoss      CELoss\n",
    "       Recall\n",
    "       \n",
    "       mixed           0.62             0.53       0.35  \n",
    "       noise           0.40             0.55       0.55\n",
    "\n",
    "\n",
    "       FPR             0.65             0.475       0.60\n",
    "       \n",
    "   \n",
    "BCEWithLogitsLoss gives highest acccuraccy but NLLoss gives lowest FPR overall.\n",
    "\n",
    "**Result: Proceed with NLLoss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output neuron looked something like this: [32, 2] matrix indicating that one neuron was assigned for. each class (k = 2). This makes it NOT a binary classification problem but a **Predicting a single label from multiple classes** problem.\n",
    "\n",
    "## Predicting a single label from multiple classes ##\n",
    "The final layer of the neural network will have one neuron for each of the classes and they will return a value between 0 and 1, which can be inferred as a probably. The output then results in a probability distribution as it sums to 1.\n",
    "To understand the accuracy of the prediction, each output is compared with its corresponding true value. True values have been one-hot-encoded meaning a 1 appears in the column corresponding to the correct category, else a 0 appears\n",
    "\n",
    "\n",
    "Softmax with cross entropy should be tried\n",
    "\n",
    "\n",
    "## Binary Classification Loss Functions [UPDATE: INVALID]\n",
    "\n",
    "[o] ***Binary Cross-Entropy:***\n",
    "Cross-entropy is the default loss function to use for binary classification problems. \n",
    "It is intended for use with binary classification where the target values are in the set {0, 1}. Mathematically, it is the preferred loss function under the inference framework of maximum \n",
    "likelihood. \n",
    "It is the loss function to be evaluated first and only changed if you have a good reason.\n",
    "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. \n",
    "The score is minimized and a perfect cross-entropy value is 0.\n",
    "\n",
    "\n",
    "[x] ***Hinge Loss:*** \n",
    "An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models.It is intended for use with binary classification where the target values are in the set {-1, 1}.\n",
    "\n",
    "[x] ***Squared Hinge Loss:*** \n",
    "Calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate. As with using the hinge loss function, the target variable must be modified to have values in the set {-1, 1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader=None,  epochs=15, save=True):\n",
    "    for epoch in range(epochs): \n",
    "        pointnet.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n",
    "\n",
    "            loss = pointnetloss(outputs, labels, m3x3, m64x64)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:\n",
    "                print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' % \n",
    "                      (epoch + 1, i + 1, len(train_loader), running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        pointnet.eval()\n",
    "        correct = total = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noted that the read method converts to floats that looses several points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a 4th dimension: https://github.com/charlesq34/pointnet/issues/213\n",
    "https://github.com/charlesq34/pointnet/issues/12\n",
    "https://github.com/fxia22/pointnet.pytorch/issues/69\n",
    "\n",
    "\n",
    "Helpful visualisations: [https://github.com/Wind-Wing/pointnet-visualization/blob/master/visualization_report.pdf]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "km3net",
   "language": "python",
   "name": "km3net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
