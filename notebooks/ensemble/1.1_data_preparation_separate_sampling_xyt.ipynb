{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xyt Train & Test Data Preparation\n",
    "\n",
    "Here the data is prepared such that 6550 points, ordered and hits first can be taken for train and random points can be taken for test\n",
    "\n",
    "Point Cloud NN requires data in the following scheme:\n",
    "\n",
    "dataset\n",
    "\n",
    "--- class1  \n",
    "\n",
    "    --- train  \n",
    "        * file.off\n",
    "        * file.off\n",
    "    \n",
    "    --- test \n",
    "        * file.off\n",
    "        * file.off\n",
    "    \n",
    "\n",
    "--- class2  \n",
    "\n",
    "    --- train \n",
    "        * file.off\n",
    "        * file.off\n",
    "    \n",
    "    --- test  \n",
    "        * file.off\n",
    "        * file.off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_stats(df_noise, df_mixed):\n",
    "    \"\"\"\n",
    "    General Stats on mixed and noise groups\n",
    "    \"\"\"\n",
    "    noise_stats = df_noise.groupby('group')['label']\\\n",
    "                    .count()\\\n",
    "                    .sort_values(ascending=False)\\\n",
    "                    .describe()\n",
    "    mixed_stats = df_mixed.groupby('group')['label']\\\n",
    "                    .count()\\\n",
    "                    .sort_values(ascending=False)\\\n",
    "                    .describe()\n",
    "    \n",
    "    neg, pos = np.bincount(df_mixed['label'])\n",
    "    total = neg + pos\n",
    "    \n",
    "    hits = df_mixed[df_mixed.label == 1].groupby(['group', 'label'])['label']\n",
    "    noise = df_mixed[df_mixed.label == 0].groupby(['group', 'label'])['label']\n",
    "    \n",
    "    hits_count = hits.count().sort_values(ascending=False)[:SIZE]\n",
    "    noise_count = noise.count().sort_values(ascending=False)[:SIZE]\n",
    "    \n",
    "    hits_to_noise = hits_count.droplevel(level='label')/noise_count.droplevel(level='label')\n",
    "    \n",
    "    class_imbalance = df_mixed.groupby(['group', 'label'])['label'].count()[:20]\n",
    "    \n",
    "        \n",
    "    print(\"NOISE STATS: \\n{}\\n\".format(noise_stats))\n",
    "    print(\"MIXED STATS: \\n{}\\n\".format(mixed_stats))\n",
    "    \n",
    "    print(\"Mixed Groups: \\n\")\n",
    "    print(\"Examples:\\n Total: {}\\n Positive: {} ({:.2f}% of total)\\n\".format(total,\n",
    "                                                                             pos, 100 * pos / total))\n",
    "    \n",
    "    print(\"Example of class imbalance in mixed groups: \\n \".format(class_imbalance.head()))\n",
    "           \n",
    "    print(\"Hits Only (Within mixed groups):\\n\")\n",
    "    print(\"The largest hits for a group: {}\\n\".format(hits_count.max()))\n",
    "    print(\"The smallest hits for a group: {}\\n\".format(hits_count.min()))\n",
    "    print(\"The mean hits for a group: {}\\n\".format(hits_count.mean()))\n",
    "                                                                                                                                    \n",
    "    print(\"Noise Only (Within mixed groups):\\n\") \n",
    "    print(\"The largest noise for a group: {}\\n\".format(noise_count.max()))\n",
    "    print(\"The smallest noise for a group: {}\\n\".format(noise_count.min())) \n",
    "    print(\"The mean noise for a group: {}\\n\".format(noise_count.mean()))\n",
    "    print(\"Hits-Noise Ratio:\\n\") \n",
    "    print(\"The highest ratio: {:.2f}% ({})\\n\".format(hits_to_noise.max()*100,\n",
    "                                                    hits_to_noise.max()))\n",
    "    print(\"The smallest ratio: {:.2f}% ({}) \\n\".format(hits_to_noise.min()*100,\n",
    "                                                       hits_to_noise.min())) \n",
    "    print(\"The mean ratio: {:.2f}% ({})\\n\".format(hits_to_noise.mean()*100, \n",
    "                                                  hits_to_noise.mean()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot(df_mixed):\n",
    "    \"\"\"\n",
    "    Generate plots of full data in mixed groups\n",
    "    \"\"\"\n",
    "    pos_df = pd.DataFrame(df_mixed[df_mixed.label == 1], columns = df_mixed.columns)\n",
    "    neg_df = pd.DataFrame(df_mixed[df_mixed.label == 0], columns = df_mixed.columns)\n",
    "    sns.jointplot(pos_df['pos_z'], pos_df['time'],\n",
    "                  kind='hex')\n",
    "    plt.suptitle(\"Positive distribution (Hits) for pos_z vs time\")\n",
    "    sns.jointplot(neg_df['pos_z'], neg_df['time'],\n",
    "                  kind='hex')\n",
    "    _ = plt.suptitle(\"Negative distribution (Noise) for pos_z vs time\")\n",
    "    \n",
    "    plt.savefig(\"../../assets/Distributon of points in Mixed Groups (posz-time)\")\n",
    "    \n",
    "\n",
    "def sampled_plot(sampled_mixed):\n",
    "    \"\"\"\n",
    "    Generate plots of equally sampled points in mixed groups\n",
    "    \"\"\"\n",
    "    list_mixed_groups = df_mixed.groupby('group')['label'].count().sort_values(ascending=False)[:SIZE]\n",
    "    data_subset = sampled_mixed.loc[sampled_mixed['group'].isin(list_mixed_groups)]\n",
    "    pos_df = data_subset[data_subset.label == 1]\n",
    "    neg_df = data_subset[data_subset.label == 0]\n",
    "    \n",
    "    sns.jointplot(pos_df['pos_z'], pos_df['time'],\n",
    "                  kind='hex')\n",
    "    plt.suptitle(\"EQUAL SAMPLING: Positive distribution (Hits) for pos_z vs time\")\n",
    "    \n",
    "    sns.jointplot(neg_df['pos_z'], neg_df['time'],\n",
    "                  kind='hex')\n",
    "    _ = plt.suptitle(\"EQUAL SAMPLING: Negative distribution (Noise) for pos_z vs time\")\n",
    "\n",
    "    plt.savefig(\"../../assets/Distributon of Equally Sampled points in Mixed Groups (posz-time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(READ_PATH):\n",
    "    \"\"\"\n",
    "    Read CSV at specified Path\n",
    "    \"\"\"\n",
    "    return pd.read_csv(READ_PATH)\n",
    "\n",
    "\n",
    "def identify_groups(df):\n",
    "    \"\"\"\n",
    "    1. Tag groups that have only noise [0]\n",
    "    2. Tag groups that have both noise and hits [0,1]\n",
    "    3. Separate noise groups and hits+noise groups\n",
    "    \"\"\"\n",
    "    # Label groups by noise and and hits\n",
    "    df_count_label_type = pd.DataFrame(df.groupby('group')['label'].unique()).reset_index()\n",
    "    \n",
    "    # Obtain groups with only hits\n",
    "    df_noise = df_count_label_type.loc[\n",
    "        np.array(list(map(len, df_count_label_type.label.values))) == 1]\n",
    "    \n",
    "    # Obtain groups with noise && hits\n",
    "    df_mixed = df_count_label_type.loc[\n",
    "        np.array(list(map(len, df_count_label_type.label.values))) > 1]\n",
    "    \n",
    "    return df_noise, df_mixed\n",
    "\n",
    "\n",
    "def generate_dfs(df, df_noise, df_mixed):\n",
    "    \"\"\"\n",
    "    Obtain full dataframe based on identified noise\n",
    "    only groups and mixed groups\n",
    "    \"\"\"\n",
    "    df_noise = df[df.group.isin(df_noise.group)]\n",
    "    df_mixed = df[df.group.isin(df_mixed.group)]\n",
    "    \n",
    "    return df_noise, df_mixed\n",
    "\n",
    "\n",
    "def identify_top_groups(df, SIZE):\n",
    "    \"\"\"\n",
    "    Obtain a list of the top groups to be selected as \n",
    "    per SIZE\n",
    "    \n",
    "    **Note: SIZE == 200**\n",
    "    \"\"\"\n",
    "    top_groups = df.groupby('group')['label'].count().sort_values(ascending=False)[:SIZE]\n",
    "    top_groups = list(top_groups.index)\n",
    "    \n",
    "    return top_groups\n",
    "\n",
    "\n",
    "def randomize_files(files):\n",
    "    \"\"\"\n",
    "    Shuffles files in a random order\n",
    "    \"\"\"\n",
    "    return shuffle(files)\n",
    "\n",
    "\n",
    "def prepare_split(files):\n",
    "    \"\"\"\n",
    "    Identifies the ratios for files to be split into\n",
    "    \n",
    "    **Note**\n",
    "    Current setting is for 80-20\n",
    "    \"\"\"\n",
    "    eighty = int(0.8 * len(files))\n",
    "    twenty = int(len(files) - eighty)\n",
    "    files = np.array(files)\n",
    "    \n",
    "    return eighty, twenty, files\n",
    "\n",
    "\n",
    "def generate_ids(eighty, twenty):\n",
    "    \"\"\"\n",
    "    Assigns 1 to 80% of the files and 0 to 20% of the files\n",
    "    \"\"\"\n",
    "    idx = np.hstack((np.ones(eighty),\n",
    "                     np.zeros(twenty)))\n",
    "    return idx\n",
    "\n",
    "\n",
    "def train_test_split(files, idx):\n",
    "    \"\"\"\n",
    "    Files tagged as 1 are categorised as training files\n",
    "    Files tagged as 0 are categorised as test files\n",
    "    \"\"\"\n",
    "    train = files[idx == 1]\n",
    "    test = files[idx == 0]\n",
    "    print(\"TRAIN SET: {0}\".format(train))\n",
    "    print(\"TEST SET: {0}\".format(test))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def generate_train_test(df):\n",
    "    top_groups = identify_top_groups(df, SIZE)\n",
    "    randomize_files(top_groups)\n",
    "    eighty, twenty, files = prepare_split(top_groups)\n",
    "    idx = generate_ids(eighty, twenty)\n",
    "    train, test = train_test_split(files, idx) \n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def sort_groups(df):\n",
    "    \"\"\"\n",
    "    Within each mixed groups (hits + noise), we need to sort\n",
    "    groups such that those with highest hits are selected first\n",
    "    \n",
    "    DataFrame --> List(int)\n",
    "    \"\"\"\n",
    "    \n",
    "    # group by groups and labels and associated counts for each label\n",
    "    grouped_df = pd.DataFrame(df.groupby(['group', 'label'])['label'].count())\n",
    "    grouped_df = grouped_df.rename(columns={'label':'count'})\n",
    "    \n",
    "    # sort groups based on highest occurance of hits\n",
    "    grouped_sorted_df = grouped_df.sort_values(grouped_df.columns.tolist())\\\n",
    "                            .sort_index(level=1, ascending=False, sort_remaining=False)\\\n",
    "                            .reset_index()\n",
    "    \n",
    "    # Obtain list of groups with highest hits based on sorted order\n",
    "    sorted_groups_list = pd.DataFrame(grouped_sorted_df.group)\n",
    "\n",
    "    # Drop duplicate groups\n",
    "    sorted_groups_list = sorted_groups_list.drop_duplicates()\n",
    "\n",
    "    # Count the occurances of groups (should only occur once)\n",
    "    sorted_groups_list['g'] = sorted_groups_list.groupby('group').cumcount()\n",
    "\n",
    "    # Make copy of dataframe\n",
    "    copy_df = df\n",
    "    \n",
    "    # Save original index positions as a column\n",
    "    copy_df_indices = copy_df.reset_index()\n",
    "    \n",
    "    # Make a count of occurances for each group\n",
    "    copy_df_indices['group_count'] = copy_df_indices.groupby('group').cumcount() \n",
    "    \n",
    "    # Merge the list of groups with the partial df to obtain corresponding full dataframe\n",
    "    copy_df = sorted_groups_list.merge(copy_df_indices)\\\n",
    "                                .set_index('index')\\\n",
    "                                .rename_axis(None)\\\n",
    "                                .drop(['group_count', 'g'], axis=1)\n",
    "    \n",
    "    # For each group, sort by labels within each group starting from 1 till 0\n",
    "    df = copy_df.groupby(['group'], sort=False)\\\n",
    "                 .apply(lambda x: x.sort_values(['label'], ascending=False))\\\n",
    "                 .reset_index(drop=True)\n",
    "    \n",
    "    return df    \n",
    "\n",
    "\n",
    "def equal_sampling(df):\n",
    "    \"\"\"\n",
    "    Equally sample points from each timeslice group.\n",
    "    Use 6550 - min number of points in hits group\n",
    "    \n",
    "    **Note** \n",
    "    Sorting values on label ensures that max number of\n",
    "    hits per group is taken. \n",
    "    \n",
    "    **For example**\n",
    "    Group 1 has 6549 noise and 600 hits. Sorting first on hits will allow\n",
    "    sample to have 600 hits and 5950 noise. \n",
    "    Without any sorting, sample would have had 6549 noise and 1 hit\n",
    "    \"\"\"    \n",
    "    POINTS = 6550\n",
    "    return df.groupby('group', sort=False)\\\n",
    "             .head(POINTS)\\\n",
    "             .reset_index(drop=True)\n",
    "\n",
    "\n",
    "def remove_groups(df):\n",
    "    \"\"\"\n",
    "    Remove groups that have less than 6550 members per timeslice\n",
    "    group\n",
    "    \"\"\"\n",
    "    POINTS = 6550\n",
    "    g = df.groupby('group')\n",
    "    \n",
    "    return g.filter(lambda x: len(x) >= POINTS)\n",
    "\n",
    "\n",
    "def sample_noise_data(tag, df):\n",
    "    \"\"\"\n",
    "    Sample noise based on specified tag\n",
    "    \"\"\"\n",
    "    if tag == \"equal\":\n",
    "        sampled_noise = equal_sampling(df)\n",
    "        equal_sampled_noise = remove_groups(sampled_noise)\n",
    "\n",
    "        return equal_sampled_noise\n",
    "    \n",
    "    \n",
    "def sample_mixed_data(tag, df):\n",
    "    \"\"\"\n",
    "    Sample mixed data based on specified tag\n",
    "    \"\"\"\n",
    "    df = sort_groups(df)\n",
    "    \n",
    "    if tag == \"equal\":\n",
    "        return equal_sampling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_noise_test(df_noise, test, WRITE_NOISE_TEST):\n",
    "    \"\"\"\n",
    "    Save unsampled noise test data\n",
    "    \"\"\"\n",
    "    for idx in test:\n",
    "        file_name = \"group_\"+str(idx)+\".xyz\"\n",
    "        np.savetxt(WRITE_NOISE_TEST + file_name,\n",
    "                   df_noise[df_noise.group == idx][['pos_x', 'pos_y', 'time']].values)\n",
    "    \n",
    "    print(\"All {0} files saved successfully in {1}!\"\\\n",
    "                  .format(len(test), WRITE_NOISE_TEST))\n",
    "    \n",
    "def save_mixed_test(df_mixed, test, WRITE_MIXED_TEST):\n",
    "    \"\"\"\n",
    "    Save unsampled mixed test data\n",
    "    \"\"\"\n",
    "    for idx in test:\n",
    "        file_name = \"group_\"+str(idx)+\".xyz\"\n",
    "        np.savetxt(WRITE_MIXED_TEST + file_name,\n",
    "                   df_mixed[df_mixed.group == idx][['pos_x', 'pos_y', 'time']].values)\n",
    "    \n",
    "    print(\"All {0} files saved successfully in {1}!\"\\\n",
    "                  .format(len(test), WRITE_MIXED_TEST))\n",
    "    \n",
    "    \n",
    "def save_noise_train(sampled_df, train, WRITE_NOISE_TRAIN):\n",
    "    \"\"\"\n",
    "    Save sampled noise train data\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx in train:\n",
    "        file_name = \"group_\" + str(idx) + \".xyz\"\n",
    "        np.savetxt(WRITE_NOISE_TRAIN + file_name,\n",
    "                   sampled_df[sampled_df.group == idx][['pos_x', 'pos_y', 'time']].values)\n",
    "    \n",
    "    print(\"All {0} files saved successfully in {1}!\"\\\n",
    "                .format(len(train), WRITE_NOISE_TRAIN))\n",
    "    \n",
    "    \n",
    "def save_mixed_train(sampled_df, train, WRITE_MIXED_TRAIN):\n",
    "    \"\"\"\n",
    "    Save sampled mixed train data\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx in train:\n",
    "        file_name = \"group_\" + str(idx) + \".xyz\"\n",
    "        np.savetxt(WRITE_MIXED_TRAIN + file_name,\n",
    "                   sampled_df[sampled_df.group == idx][['pos_x', 'pos_y', 'time']].values)\n",
    "    \n",
    "    print(\"All {0} files saved successfully in {1}!\"\\\n",
    "                .format(len(train), WRITE_MIXED_TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_PATH = \"../../data/simplified_data.csv\"\n",
    "WRITE_NOISE_TRAIN = \"../../data/ensemble/points/noise/train/\"\n",
    "WRITE_NOISE_TEST = \"../../data/ensemble/points/noise/test/\"\n",
    "\n",
    "WRITE_MIXED_TRAIN =  \"../../data/ensemble/points/mixed/train/\"\n",
    "WRITE_MIXED_TEST = \"../../data/ensemble/points/mixed/test/\"\n",
    "\n",
    "SIZE = 200\n",
    "\n",
    "df = read_csv(READ_PATH)\n",
    "df_noise, df_mixed = identify_groups(df)\n",
    "df_noise, df_mixed = generate_dfs(df, df_noise, df_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET: [2385  990  432  914 1567 1189 1481 3060 3974 5511 3355 5067 2621 2128\n",
      "  686 5308    0 5905 5434 3941 6555 3042 1202  788 6132 4647 4477 2093\n",
      " 6433  416 5605 4590 6589 4391 3492 3609  347 3222 3189 1016  474  648\n",
      " 4019 1325 3416  437 5097 2834 2414 6241 1846 1204 5009  134 6630 5149\n",
      "  208 5922 2932 1681 6333 5038  301 4848 6117 2533 6436 1304 3507 5648\n",
      " 1639 4199 3219 5678 5222 4615  601 4032 2272 5718 5412 2957 5510 5550\n",
      "  762 3123 2197 4969 6177 3594 6540  449  909 4870 5840 1333 1729 4775\n",
      " 6432 5982 4267 2956 3704 2875 4796 4900 6048 2527 5977 6139 2872 3152\n",
      " 2719 6560 5182  630 1413 6396 4081  190 1620 6181 5941 4591 2934 5991\n",
      " 1586 3440  999  758 4897  374 1246 3487 1955 2869 1055 4671 4426 1346\n",
      " 3968 4060 4444 3215 5103 3799 3925 3815 3352 3403  669 1129 1531 3168\n",
      " 3758  210 1374 4571 3569 1667]\n",
      "TEST SET: [4430 3985 3943 5190 1391 3792 4483 4179 1405 4695 2751 3253 2658 4328\n",
      " 3340 4274 3180 3960 4640 4799 1108  576 3763 5578 4989 4463 1151 1583\n",
      " 5337 2557 5701 6427 5491 1101 5006  532  583 4874 2587  903]\n",
      "All 40 files saved successfully in ../../data/ensemble/points/noise/test/!\n",
      "All 160 files saved successfully in ../../data/ensemble/points/noise/train/!\n"
     ]
    }
   ],
   "source": [
    "####NOISE####\n",
    "train, test = generate_train_test(df_noise)\n",
    "df_train_noise = df_noise[df_noise.group.isin(train)]\n",
    "df_train_noise = sample_noise_data(\"equal\", df_train_noise)\n",
    "\n",
    "save_noise_test(df_noise, test, WRITE_NOISE_TEST)\n",
    "save_noise_train(df_train_noise, train, WRITE_NOISE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET: [6567 2161 4034 2795 6221 6038 6493 5234 1637 1836 3094 3670  411  231\n",
      " 5295 3845 5032 5133 5280 5114 4562  797 1937 5886 3287  281  988 2890\n",
      " 2099 4646 4327  554 1154 4132 1926 2711 6134 4515 4075 4532  809  979\n",
      "  166 6109  249 6084 4080 6426 6588 1653 6542 2362 2338 1657 2098 5847\n",
      " 1194 3490 2231 4735  845 1184  941 2821 5219  831 5272 5747 1273 3063\n",
      " 5312 1446 1454   50 5010 4227 5857 3514 1584  584 1275  530 4717 6378\n",
      " 6534 4791 4926 4653 2389 5823 5270 4958 2517 2774 3924   32 3628 6314\n",
      "  355 3072 3953 3483 6553   36 5422 2374 1599  491 5659 3398 1650 3025\n",
      " 1820 2367 4569 1357 4793 5866 5725 1484 6495 2330 3700 6006 1671 5281\n",
      " 3727 2323 4943 2211 4347 5799 4215 1813 2759 1209 3923  103  997 4679\n",
      "  394 2024 4460 1516 4428 5116 2567 5883 4740  102  716 5795 4565 2021\n",
      " 4243 3310 1276 1918 2642 1973]\n",
      "TEST SET: [1344 2404 1825 4282 1658 2185 1893 4301 1163 1496 2654 6337 1172 5621\n",
      " 1232 5812 1732 3315 3252  810 3170 4417  375  857 5215 3611 6203 6590\n",
      " 1869 6530 1214 1038 1488  615 3967  477   55 3001 5211 4854]\n",
      "All 40 files saved successfully in ../../data/ensemble/points/mixed/test/!\n",
      "All 160 files saved successfully in ../../data/ensemble/points/mixed/train/!\n"
     ]
    }
   ],
   "source": [
    "####MIXED####\n",
    "train, test = generate_train_test(df_mixed)\n",
    "df_train_mixed = df_mixed[df_mixed.group.isin(train)]\n",
    "df_train_mixed = sample_mixed_data(\"equal\", df_train_mixed)\n",
    "\n",
    "save_mixed_test(df_mixed, test, WRITE_MIXED_TEST)\n",
    "save_mixed_train(df_train_mixed, train, WRITE_MIXED_TRAIN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "km3net",
   "language": "python",
   "name": "km3net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
